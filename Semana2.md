# Agentes ~~no tan~~ Inteligentes
Anteriormente (en las notas pasadas), me dediqué a hablar meramente de la historia (con algunos malos chistes) de la inteligencia artificial, desde concepciones más terrenales donde iniciaban con la misma interrogante acerca del pensamiento, pasando con estudiosos como Alan Turing (una persona con un humor bastante "inglés"), hasta llegar a científicos más contemporáneos que comparten algo en común, ~~no tener pinche tele~~ la curiosidad del pensamiento.
Ahora si entraremos en materia con respecto a conceptos de Inteligencia Artificial, con algo bastante elemental como es un agente y un entorno.

## Agentes y Entornos
Un **agente** es algo (entidad beligna, amorfa, maligna si quieres, etc.) que puede tener una percepción dentro de un **entorno** donde mediante **sensores** y **actuadores** puede desempeñarse y realizar una gran cantidad de cosas. La utilización del término *percepción* se refiere al contenido que el agente se encuentre percibiendo durante un lapso de tiempo. Esta *secuencia de percepción* es una historia sobre todo lo que el agente ha percibido dentro de su entorno (así de acartonado y ~~marihuano~~ como suena). Esto lo podemos generalizar como:

> La selección que un agente realice en una instancia determinada puede depender de su conocimiento pre-existente y en todo lo observado que se encuentra en su secuencia de percepción hasta el momento, pero no en nada que no se haya percibido.

Matemáticamente hablando, podemos decir que el comportamiento de un agente se describe por la **función del agente** que muestra cualquier secuencia de percepción dada a una acción.
Si nos ponemos bien truchas con lo anterior, el agente es como tu amigo que ya anda pasado de caguamas y apuestas diciendole *Corre cinco vueltas a la cuadra para que se te baje la peda y te pago el uber camino a casa*. Entonces el wey bien emocionado va a correr, percibiendo un entorno (la cuadra donde estaban pisteando) y tiene un objetivo en mente (correr como pendejo a cambio de que le pages el uber a su casa), su secuencia de percepción es todo por lo que está pasando el bato con tal de lograr tal proeza (correr en círculos, sudando lo que se tomó en caguamas para no pagar el uber a su casa, pues).

Retomando de nuevo el ejemplo, ¿nuestro borracho amigo puede correr las 5 vueltas a la cuadra de diferentes maneras, no? El wey capaz que tiene un pensamiento algorítmico (es sorprendente lo que un borracho puede hacer) y piensa en una serie de pasos en como aguantar la proeza de correr como pendejo en círculos en una cuadra y no sudar la gota gorda. Esto ya en términos un poco más formales, es una perspectiva de como le podemos dar dotes a un agente para que interactúe en un entorno dado.

## El buen comportamiento: El concepto de racionalidad
Un **agente racional** es aquel que hace lo correcto. Obviamente que hacer lo correcto es mejor que hacer lo incorrecto (por más trivial que se entienda). Pero como nos podemos asegurar que nuestro agente *está haciendo lo correcto*. Pues es bastante simple que lo voy a poner así:

> Un agente está haciendo lo correcto cuando hace lo que queremos que haga.

Digamos que para el disfortunio de los conspiranoicos, el concepto de "hacer lo correcto" para los agente es en referencia a una noción llamada **consecuencialismo**, donde evaluamos el comportamiento del agente por sus consecuencias (la consecuencia de si hace o no lo que queremos que haga). Entonces el disfrotunio recae en que dependiendo de lo que "sea correcto" para el diseñador del agente, es lo que será correcto para el agente. Esto quiere decir que podemos decidir si queremos que nuestro agente se comporte como un promotor de la paz mundial o como un completo racista, sexista, misógino, etc. Claramente, nosotros somos personas de bien y queremos que nuestros agentes hagan cosas útiles y no las otras chingaderas que mencioné. El aseguramiento de que nuestro agente realice las cosas que queremos debe de ser medible gracias a su secuencia de percepciones.

Podemos establecer métricas de desempeño para la medición del desempeño:

> Como una regla general, es mejor diseñar metricas de desempeño de acuerdo a lo que se quiere lograr en el entorno, que acorde a como uno piensa que el agente se debe de comportar

Esto quiere decir que, conociendo el entorno donde nos encontramos, para poder contabilizar el desempeño de nuestro agente, lo mejor es pensar en como se encuentra en entorno y así realizar mediciones, que simplemente tener en mente un "supuesto" comportamiento que queremos que tenga. Cabe aclarar, es necesario que el agente haga lo que queremos que haga, pero también una parte fundamental es el hecho de que pueda interactuar con el entorno en el que se encuentra realizando la menor cantidad de perturbaciones posibles.

Entonces, podemos llegar a la definición formal de agente racional:

> Para cada posible secuencia de percepción, un agente racional deberá de seleccionar una acción que maximice sus metricas esperadas, dada una evidencia provista por la secuencia de percepción y cualesquier conocimiento previamente adquirido que el agente tenga.

Esta ~~marihuanada~~ quiere decir que, nuestro agente deberá de seleccionar la acción que *genere más utilidad* de acuerdo a lo que se espera de el (que haga lo correcto) con información que previamente le damos (las reglas de como queremos que haga las cosas) dentro del entorno donde se encuentra, esto es lo que conforma el concepto de racionalidad en el campo de la Inteligencia Artificial.

## Omnisciencia, aprendizaje y autonomía
Debemos de ser cuidadosos realizando la distinción entre la racionalidad y la omnisciencia. Un agente que es *omnisciente* conoce el resultado de sus acciones y por lo tanto puede actuar acorde a eso, pero en la realidad, el concepto de omnisciencia es imposible en nuestro plano. Es como decir lo siguiente retomando el ejemplo de nuestro amigo pasado de caguamas: Mientras el se encuentra corriendo las vueltas para que le pagues el uber a su casa, de repente por razones *omniscentes*, un avión Boeing 737 MAX pierde su puerta de emergencia en medio vuelo (cualquier parecido con la realidad es mera coincidencia: https://www.bbc.com/news/world-us-canada-67899564), donde casualmente iba pasando por la ciudad donde vivimos y le cae encima. ¿Esto lo podríamos considerar como algo irracional? El obituario del pobre amigo no dirá necesariamente que murió por correr por dinero borracho en la calle sinceramente.

Este ejemplo demuestra que la racionalidad no es lo mismo que la perfección. La racionalidad maximiza el desempeño *esperado*, mientras que la *perfección* maximiza el desempeño actual. Preguntarse si los agentes deberían ser perfectos es una pregunta injusta para estos, ya que simplemente el hecho de que la acción esperada que ellos realicen sea la acción ideal, se torna imposible en temas de diseño.

Para que nuestros agentes realicen acciones y mejoren su desempeño, es importante que estos tengan la habilidad de recopilar información, siendo esto una parte importante del concepto de racionalidad que será cubierto más adelante. Un ejemplo simple de como un agente pudiera recopilar información sería mediante la exploración que puede realizar del entorno en donde se encuentra. Nuestra definición requiere que nuestro agente racional no solo recopile información a lo pendejo del entorno, sino que también **aprenda de este** lo más que pueda, de tal manera que nuestro agente empiece a ganar experiencia (Como en el Final Fantasy o cualquier juego de los Nintendos) para que este se modifique y mejore.

Es de total menester comentar también que a pesar de que el agente tenga como una de sus herramientas el conocimiento predefinido y adquirido posteriormente por cualquier exploración para su aprendizaje, esto nos ayuda a explicar (o tal vez, darlo por hecho) que el agente debe de tener **autonomía**. Esto quiere decir que independientemente de lo que aprenda en su viaje espiritual o el conocimiento que le dimos previamente en caso de ser necesario, pueda entender que se puda compensar de estos conocimientos para que ejerzca esta autonomía.

## La Naturaleza de los Entornos
Después de pegarnos un viaje explorando la racionalidad en los agentes, es hora de pegar otro viaje con respecto a los entornos. Primero, debemos de pensar en algo llamado **tareas de los entornos**, que son esencialmente los "problemas" en donde los agentes racionales son la "solución". Podemos comenzar mostrando como especificar una tarea de entorno ilustrando el proceso mediante algunos ejemplos. Después, podemos mostrar que las tareas de entorno pueden venir en una variedad de sabores. La naturaleza de las tareas de entorno afecta directamente el diseño del programa de nuestro agente, esto quiere decir, que dependiendo del entorno, es el agente.

### Especificando la tareas de los entornos
Retomando lo discutido para conceptualizar el tema de la racionalidad de nuestro agente, es de mi alegría mencionar que podemos pensarlo en un interesante acrónimo para la fácil comprensión. En inglés, lo encontraríamos como PEAS (Performance, Environment, Actuators, Sensors). En nuestra lengua suena mucho más fácil de recordar porque es un acto mundano de la vida misma, MEAS (Medición, Entorno, Actuadores, Sensores). Ahora bien, en que consiste cada parte de nuestro ultra decente acrónimo:
- **Medición**: ¿Qué es lo que consideraremos como un buen desempeño? Esta pregunta es extrapolable dependiendo del agente que estemos creando para el entorno donde se encontrará. Un ejemplo de esto podría ser un vehículo autónomo, en donde una medida de desempeño sería que tanto combustible consumió, si llegó al destino correcto, etc.
- **Entorno**: ¿En qué entorno nos vamos a encontrar? Retomando el ejemplo del vehículo autónomo, esto podría ser la variedad de boulevares, avenidas, calles, etc. El tamaño de estas calles, la densidad del tráfico en el momento que se encuentra círculando y muchos otros factores que moldean el alrededor en donde el vehículo se encuentra.
- **Actuadores**: Esto lo podemos ver (retomando la analogía del vehículo autónomo) como la manera que podemor controlar ciertos factores como la aceleración, la dirección y el frenado.
- **Sensores**: Podemos verlo como los "ojos" del agente, para que este pueda apreciar el entorno. En el ejemplo del vehículo autónomo, sería que el vehículo contara con cámaras en todos los lugares necesarios para tener completa noción de los alrededores.

### Propiedades de las tareas de entorno
Existen una serie de propiedades que debemos de resaltar con la finalidad de poder diseñar nuestros agentes racionales para el entorno donde lo queremos, las cuáles son:
- **Completamente observable vs Parcialmente observable**: Si los sensores de un agente tienen completo acceso al estado del entorno siempre, se dice que la tarea del entorno es *completamente observable*. Esto quiere decir que el agente puede detectar todos los aspectos relevantes para realizar una decisión en base a esto. Si lo que sucede es lo contrario a todo esto, quiere decir que el sensor es *parcialmente observable* o inclusive *no observable*.
- **Un solo agente vs multiagente**: La distinción entre entornos con un solo agente y entornos con multiagentes puede sonar bastante simplista. Pero podemos tener situaciones en donde contemos con más de un agente en el entorno. Un ejemplo de esto podría ser una partida de ajedrez.
- **Determinista vs No Determinista**: Si podemos realizar una secuencia clara de las acciones del agente dentro del entorno, esto quiere decir que nuestro entorno es totalmente determinista. En cambio si no podemos decir que nuestro agente está realizando una secuencia dentro del entorno, quiere decir que el entorno en el que el agente se encuentra es no determinista. También, para evitar confusiones; cuando hablamos de algo no determinista, lo hacemos desde una perspectiva no cuantificada. Si en cambio, utilizamos conceptos probabilisticos, estamos ante un entorno **estocástico**.
- **Episódico vs Secuencial**: En un entorno episódico, la experiencia del agente se divide en episodios *atómicos*. En cada episodio, el agente recibe un percepción y después realiza una acción. Crucialmente, el siguiente episodio no depende de las acciones tomadas en episodios previos. En cambio, en un entorno secuencial, una decisión puede afectar a las subsecuentes.
- **Estático vs Dinámico**: Si un entorno puede cambiar cuando el agente se encuentra deliberando, estamos ante un entorno que es *dinámico* para el agente. En cambio, los entornos *estáticos* son fáciles de lidiar debido a que el agente no necesita encontrarse atento a su alrededor (entorno) mientras se encuentra decidiendo una acción a realizar.
- **Discreto vs Continuo**: La distinción entre lo discreto y lo continuo aplica al *estado* del entorno, es decir, por como el tiempo es manejado y a las acciones y percepciones del agente. Por ejemplo, un juego de ajedrez tiene un número finito de estados distintos, también tiene un conjunto finito de percepciones. En cambio, un vehículo autónomo tiene acciones que se realizan *continuamente* como girar a la derecha o la izquierda. Si bien, la información sensorial (sus cámaras) podría considerarse discreta mamonamente hablando, podemos considerar que está representando variaciones de continuas con respecto a la intensidad y a la localización.
- **Conocido vs Desconocido**: La distinción no se refiere estrictamente al entorno, sino al "estado de conocimiento" del agente. Por ejemplo, en el solitario, un agente no conoce la *totalidad* de las cartas que se encuentran en el juego debido a su naturaleza en el entorno, pero el agente puede conocer la totalidad del reglamente del juego. Aquí es importante realizar la distinción que conocido vs desconocido no es lo mismo que el apartado de entornos observables vs no observables.

## La estructura de los agentes
El trabajo de la Inteligencia Artifical es diseñar programas agentes que implementen funciones, en donde dada una percepción se realize una acción. Podemos asumir que este programa *correrá* en alguna especie de máquina arcana diabólicamente furiosa (una computadora). Esta máquina arcana diabólicamente furiosa cuenta con sus sensores y actuadores los cuales podemos considerar como la *arquitectura del agente*.

Si bien esto suena bastante bueno para ser verdad (porque hay que encontrar el hilo negro a todo), podemos decir lo siguiente acerca del rubro de la Inteligencia Artificial:

> El verdadero reto de la Inteligencia Artificial es encontrar la manera de escribir programas que, en la extensión de lo posible, produzca pensamiento racional de *pequeños programitas* que de un tabulador.

Podemos realizar un listado de cuatro tipo de agentes los cuáles son:
- **Agentes basados en reflejo**: Las acciones de estos agentes son basadas en la *percepción del momento*, ignorando el resto. Simplemente, es un agente que reacciona a algo que ocurre en el momento.
- **Agente basado en modelo**: Este agente mantiene un *historial* de sus acciones que no puede realizar en el momento, con la finalidad de realizar esas acciones después o cuando *sea el tiempo prudente de hacerlo*.
- **Agente basado en metas**: Este agente se encuentra enfocado en *lo que va a lograr*. Lo cuál sirve bastante dependiendo del entorno en donde se encuentre debido a que son bastante al grano a diferencia de los que son basados en modelo. Sin embargo, hay situaciones en donde este agente no se considera idóneo por su *particularidad maquiavélica* de realizar las cosas.
- **Agente basado en utilidad**: A veces lo que se va a lograr no es suficiente para generar un comportamiento de alta calidad dentro de un entorno. A veces, es necesario que en base al desempeño, tengamos que decidir entre dos caminos en donde (a) realizar una acción tendrá un alto precio o (b) realizar esta otra acción tendrá un menor precio. Con el cuento de *maximizar la utilidad*, este agente tomará una decisión que considere como *útil* en la circunstancia determinada. ¿Pero cómo podemos determinar la utilidad? Simplemente, tendiendo una expectiva de lo que *sería útil* dentro de las reglas especificadas en el agente y esto se logra mantendiendo un historial de las tareas en el entorno con su autonomía, una buena percepción, realizando un buen aprendizaje, etc.
- **Aprendices**: Al aprendiz lo podemos dividir en cuatro componentes conceptuales: (1) el elemento del aprendizaje, (2) el elemento de desempeño, (3) la crítica y (4) el generador de problemas. El elemento de aprendizaje consiste en la responsabilidad de realizar mejores. El elemento de desempeño consiste en la responsabilidad de seleccionar acciones externas tomando en cuenta lo aprendido. La crítica consiste en como el agente se encuentra trabajando y determinar que aspectos del desempeño deberán de cambiar para el futuro. El generador de problemas consiste en la responsabilidad de sugerir acciones que puedan llevar a nuevas experiencias informativas.

Es importante también conceptualizar el hecho de que un agente puede percibir un **premio**. Un ejemplo simple de esto podrían ser las propinas que un cliente del servicio de taxis autónomos dejen por buen comportamiento. Esto con finalidad de aprender que si se realiza una serie de acciones (un manejo suave del acelerador y del freno, lo que se traduce como manejar como una persona decente), pueda percibirse como la norma de hacer las cosas. También es posible asociar un **castigo** cuando el caso contrario es el que ocurre (no hay propina por ser culero, pues).

## Los componentes de los programas agente
Existen tres maneras de representar los estados y transiciones de un agente, las cuales son:
- **Representación atómica**: Son representaciones que no cuentan con estructura interna. Los estados actúan como *cajas negras* en donde solo existen transiciones sin contexto más que el hecho secuencial de las mismas. Un ejemplo de representación sería un auto que maneja de un país a otro, en donde las ciudades entre esos países son las *cajas negras* sin contexto, pero por las que hay que pasar.
- **Representación de factor**: Consisten en un conjunto finito de variables o atributos en donde cada uno tienen un valor. Aquí ya contamos con *algunos* detalles de las acciones. Retomando el ejemplo anterior, ahora contamos que entre las ciudades, podemos detenernos en algún mirador o una gasolina dependiendo si tenemos alguna necesidad diversa.
- **Representación estructurada**: Podemos ver esta representación como el paradigma orientado a objetos de la programación, en donde tenemos conjuntos de objetos de las cosas con las que podemos jugar al respecto. En el ejemplo utilizado por los dos anteriores conceptos, podríamos utilizarlo en el sentido de ir en la carretera entre las ciudadaes, entre los países y encontramos un venado lampareado en el camino, en donde tenemos las opciones a reaccionar (Tener distancia para frenar y no llevarnos al venado lampareado pues).

## Conclusiones
Aquí ya se nota que ya entramos en materia, donde podemos apreciar un abánico de conceptos diversos que se ponen bastante fumados en algunos casos, pero es entendible tomando en cuenta toda la cantidad de investigaciones y cuestionamientos que se han realizado debido a la curiosidad de querer comprender el concepto de como pensamos y como posteriormente nos hemos realizado la pregunta sobre si las máquinas diabólicamente furiosas, también pueden pensar.